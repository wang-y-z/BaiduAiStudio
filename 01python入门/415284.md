```python
!pip install bs4
!pip install xlwt
!pip install xlrd
```

    Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/
    Requirement already satisfied: bs4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.0.1)
    Requirement already satisfied: beautifulsoup4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bs4) (4.9.0)
    Requirement already satisfied: soupsieve>1.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from beautifulsoup4->bs4) (2.0)
    Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/
    Requirement already satisfied: xlwt in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (1.3.0)
    Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/
    Requirement already satisfied: xlrd in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (1.2.0)



```python
import sys
from bs4 import BeautifulSoup
import re
import urllib
import xlwt
import requests
```


```python
#得到页面全部内容   request过期，请求豆瓣返回反爬虫error 418 所以改用新的库即可获取json格式数据
def askURL(url):
    html=""
    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36'}
    #res = 
    request = urllib.request.Request(url)#发送请求
    try:
        response = urllib.request.urlopen(request)#取得响应
        html = response.read()#获取网页内容
        print ("%s crawl success" % url)
    except urllib.error.URLError as e:
        print ("%s crawl fail" % url)
        if hasattr(e,"code"):
            print (e.code)
        if hasattr(e,"reason"):
            print (e.reason)
    return html
```


```python
def askurl(url):
    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36'}
    response = requests.get(url,headers = headers)
    print(response.content.decode())
    return response.content.decode()
```


```python
#获取相关内容
def getData(baseurl):
    findLink=re.compile(r'<a href="(.*?)">')#找到影片详情链接
    findImgSrc=re.compile(r'<img.*src="(.*?)"',re.S)#找到影片图片
    findTitle=re.compile(r'<span class="title">(.*)</span>')#找到片名
    #找到评分
    findRating=re.compile(r'<span class="rating_num" property="v:average">(.*)</span>')
    #找到评价人数
    findJudge=re.compile(r'<span>(\d*)人评价</span>')
    #找到概况
    findInq=re.compile(r'<span class="inq">(.*)</span>')
    #找到影片相关内容：导演，主演，年份，地区，类别
    findBd=re.compile(r'<p class="">(.*?)</p>',re.S)
    #去掉无关内容
    remove=re.compile(r'|\n|</br>|\.*')
    datalist=[]
    for i in range(0,10):
        url=baseurl+str(i*25)
        content=askurl(url)
        soup = BeautifulSoup(content, "html.parser")
        for item in soup.find_all('div',class_='item'):#找到每一个影片项
            data=[]
            item=str(item)#转换成字符串
            # 影片详情链接
            link=re.findall(findLink,item)[0]
            data.append(link)#添加详情链接  
            imgSrc=re.findall(findImgSrc,item)[0]
            data.append(imgSrc)#添加图片链接
            titles=re.findall(findTitle,item)
            #片名可能只有一个中文名，没有外国名
            if(len(titles)==2):
                ctitle=titles[0]
                data.append(ctitle)#添加中文片名
                otitle=titles[1].replace("/","")#去掉无关符号
                data.append(otitle)#添加外国片名
            else:
                data.append(titles[0])#添加中文片名
                data.append(' ')#留空
                
            rating=re.findall(findRating,item)[0]
            data.append(rating)#添加评分
            judgeNum=re.findall(findJudge,item)[0]
            data.append(judgeNum)#添加评论人数
            inq=re.findall(findInq,item)
            #可能没有概况
            if len(inq)!=0:
                inq=inq[0].replace("。","")#去掉句号
                data.append(inq)#添加概况
            else:
                data.append(' ')#留空
            bd=re.findall(findBd,item)[0]
            bd=re.sub(remove,"",bd)
            bd=re.sub('<br(\s+)?\/?>(\s+)?'," ",bd) #去掉<br >
            bd=re.sub('/', " ",bd)#替换/
            data.append(bd.strip())
            datalist.append(data)
    return datalist
```


```python
#将相关数据写入excel中
def saveData(datalist,savepath):
    book=xlwt.Workbook(encoding='utf-8',style_compression=0)
    sheet=book.add_sheet('豆瓣电影Top250',cell_overwrite_ok=True)
    col=('电影详情链接','图片链接','影片中文名','影片外国名',
                '评分','评价数','概况','相关信息')
    for i in range(0,8):
        sheet.write(0,i,col[i])#列名
    for i in range(0,250):
        if datalist:
            data=datalist[i]
            for j in range(0,8):
                sheet.write(i+1,j,data[j])#数据
    book.save(savepath)#保存
```


```python
def main():
    print ("开始爬取......")
    baseurl='https://movie.douban.com/top250?start='
    datalist=getData(baseurl)
    savapath=u'/home/aistudio/work/豆瓣电影Top250.xls'
    saveData(datalist,savapath)
```


```python
main()
print ("爬取完成，请查看.xls文件")
```

    爬取完成，请查看.xls文件


```python
import pandas as pd
df = pd.read_excel("/home/aistudio/work/豆瓣电影Top250.xls")
print(df.head())
print(df.info())

```

                                          电影详情链接  \
    0  https://movie.douban.com/subject/1292052/   
    1  https://movie.douban.com/subject/1291546/   
    2  https://movie.douban.com/subject/1292720/   
    3  https://movie.douban.com/subject/1295644/   
    4  https://movie.douban.com/subject/1292063/   
    
                                                    图片链接    影片中文名  \
    0  https://img3.doubanio.com/view/photo/s_ratio_p...   肖申克的救赎   
    1  https://img3.doubanio.com/view/photo/s_ratio_p...     霸王别姬   
    2  https://img9.doubanio.com/view/photo/s_ratio_p...     阿甘正传   
    3  https://img3.doubanio.com/view/photo/s_ratio_p...  这个杀手不太冷   
    4  https://img3.doubanio.com/view/photo/s_ratio_p...     美丽人生   
    
                            影片外国名   评分      评价数              概况  \
    0    The Shawshank Redemption  9.7  1989087          希望让人自由   
    1                              9.6  1472240            风华绝代   
    2                Forrest Gump  9.5  1507339        一部美国近现代史   
    3                        Léon  9.4  1701191  怪蜀黍和小萝莉不得不说的故事   
    4             La vita è bella  9.5   950417           最美的谎言   
    
                                                    相关信息  
    0  导演: 弗兰克·德拉邦特 Frank Darabont   主演: 蒂姆·罗宾斯 Tim R...  
    1  导演: 陈凯歌 Kaige Chen   主演: 张国荣 Leslie Cheung   张...  
    2  导演: 罗伯特·泽米吉斯 Robert Zemeckis   主演: 汤姆·汉克斯 Tom ...  
    3  导演: 吕克·贝松 Luc Besson   主演: 让·雷诺 Jean Reno   娜塔...  
    4  导演: 罗伯托·贝尼尼 Roberto Benigni   主演: 罗伯托·贝尼尼 Robe...  
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 250 entries, 0 to 249
    Data columns (total 8 columns):
    电影详情链接    250 non-null object
    图片链接      250 non-null object
    影片中文名     250 non-null object
    影片外国名     250 non-null object
    评分        250 non-null float64
    评价数       250 non-null int64
    概况        250 non-null object
    相关信息      250 non-null object
    dtypes: float64(1), int64(1), object(6)
    memory usage: 15.7+ KB
    None



```python
'''
import matplotlib.pyplot as plt
import matplotlib

matplotlib.rcParams['font.size']=20
plt.figure(figsize=(20,5))
plt.subplot(1,2,1)
plt.scatter(df['评分'],range(1,251))
plt.xlabel('score')
plt.ylabel('rank')
#修改y轴为倒序
plt.gca().invert_yaxis()
#集中趋势的直方图
plt.subplot(1,2,2)
plt.hist(df['评分'],bins=15)
'''
```




    "\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nmatplotlib.rcParams['font.size']=20\nplt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nplt.scatter(df['评分'],range(1,251))\nplt.xlabel('score')\nplt.ylabel('rank')\n#修改y轴为倒序\nplt.gca().invert_yaxis()\n#集中趋势的直方图\nplt.subplot(1,2,2)\nplt.hist(df['评分'],bins=15)\n"




```python

```
